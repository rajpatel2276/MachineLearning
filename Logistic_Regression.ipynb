{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3486b897-878f-447d-8ed4-05f1fe2b873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf838ac-fdca-4e91-ab29-726e8c6c7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a836fcf-0300-4526-a618-f04a8a91075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data_Folder/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e913a9c6-e3b8-4ca8-8436-d0927143544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06e99d-23d0-45f0-8597-30dc805753fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04f735b-09a8-4ae0-8037-a091ac2a2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the numerical gaps\n",
    "median_age = df['age'].median()\n",
    "df['age'] = df['age'].fillna(median_age)\n",
    "# Droping missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b515cc-9abe-45ef-8ca0-a482836da8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['familysize'] = df['sibsp'] + df['parch']\n",
    "df = df.drop(columns=['sibsp','parch'])\n",
    "\n",
    "# 1.Binary Encoding for the sex column\n",
    "df['sex'] = df['sex'].map({'male':0,'female':1})\n",
    "\n",
    "# 2.One hot encoding for embarked column\n",
    "df = pd.get_dummies(df,columns=['embarked'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2113498e-4bee-4c4f-8b8d-7a5471dc2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_scale = ['age','fare','familysize']\n",
    "\n",
    "for col in cols_scale:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    df[col] = (df[col] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d0a69d-5a34-4cb2-9bf6-9ef1604fdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pclass','sex','age','fare','familysize','embarked_Q','embarked_S']\n",
    "X = df[features].values\n",
    "y = df['survived'].values\n",
    "\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af245b3e-172d-49bb-b864-41c69ec0e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self,learning_rate=0.01,iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "    def _sigmoid(self,z):\n",
    "        if not hasattr(np, \"exp\"):\n",
    "            raise TypeError(\"np is not NumPy! Did you overwrite it?\")\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        n_samples,n_features = X.shape\n",
    "        y = y.reshape(-1,1)\n",
    "\n",
    "        self.weights = np.zeros((n_features,1))\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            linear_model = np.dot(X,self.weights) + self.bias\n",
    "\n",
    "            # Applying sigmoid function\n",
    "            y_pred = self._sigmoid(linear_model)\n",
    "\n",
    "            # Cost Calculation (log loss)\n",
    "            epsilon = 1e-15\n",
    "            y_pred_clipped = np.clip(y_pred,epsilon,1-epsilon)\n",
    "            cost = -(1/n_samples) * np.sum(y*np.log(y_pred_clipped)+(1-y) * np.log(1-y_pred_clipped))\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "            # Backward pass\n",
    "            error = y_pred - y\n",
    "            dw = (1/n_samples) * np.dot(X.T, error)\n",
    "            db = (1/n_samples) * np.sum(error)\n",
    "\n",
    "            # updating the parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iter {i}: Cost {cost:.4f}\")\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        # CHANGE 3: Convert Probability to Class (0 or 1)\n",
    "        # If probability > 0.5, class is 1. Else 0.\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f6556a-f520-4a10-aeb1-45e7cddb97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = 700\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e7f5fb-f249-468f-ae94-8bbd587a7c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 202 passengers...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training on {len(X_train)} passengers...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497ded3-cff7-4913-8efc-7f9fea737272",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8411e676-5f54-400c-90eb-d441e17115a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Cost 0.6931\n",
      "Iter 100: Cost 0.5299\n",
      "Iter 200: Cost 0.4917\n",
      "Iter 300: Cost 0.4753\n",
      "Iter 400: Cost 0.4671\n",
      "Iter 500: Cost 0.4625\n",
      "Iter 600: Cost 0.4596\n",
      "Iter 700: Cost 0.4577\n",
      "Iter 800: Cost 0.4562\n",
      "Iter 900: Cost 0.4551\n",
      "Iter 1000: Cost 0.4542\n",
      "Iter 1100: Cost 0.4534\n",
      "Iter 1200: Cost 0.4527\n",
      "Iter 1300: Cost 0.4521\n",
      "Iter 1400: Cost 0.4515\n",
      "Iter 1500: Cost 0.4510\n",
      "Iter 1600: Cost 0.4505\n",
      "Iter 1700: Cost 0.4500\n",
      "Iter 1800: Cost 0.4496\n",
      "Iter 1900: Cost 0.4492\n",
      "Iter 2000: Cost 0.4489\n",
      "Iter 2100: Cost 0.4485\n",
      "Iter 2200: Cost 0.4482\n",
      "Iter 2300: Cost 0.4479\n",
      "Iter 2400: Cost 0.4476\n",
      "Iter 2500: Cost 0.4473\n",
      "Iter 2600: Cost 0.4471\n",
      "Iter 2700: Cost 0.4468\n",
      "Iter 2800: Cost 0.4466\n",
      "Iter 2900: Cost 0.4464\n",
      "Iter 3000: Cost 0.4462\n",
      "Iter 3100: Cost 0.4460\n",
      "Iter 3200: Cost 0.4458\n",
      "Iter 3300: Cost 0.4457\n",
      "Iter 3400: Cost 0.4455\n",
      "Iter 3500: Cost 0.4454\n",
      "Iter 3600: Cost 0.4452\n",
      "Iter 3700: Cost 0.4451\n",
      "Iter 3800: Cost 0.4450\n",
      "Iter 3900: Cost 0.4449\n",
      "Iter 4000: Cost 0.4447\n",
      "Iter 4100: Cost 0.4446\n",
      "Iter 4200: Cost 0.4445\n",
      "Iter 4300: Cost 0.4444\n",
      "Iter 4400: Cost 0.4443\n",
      "Iter 4500: Cost 0.4443\n",
      "Iter 4600: Cost 0.4442\n",
      "Iter 4700: Cost 0.4441\n",
      "Iter 4800: Cost 0.4440\n",
      "Iter 4900: Cost 0.4440\n"
     ]
    }
   ],
   "source": [
    "model = MyLogisticRegression(learning_rate=0.1,iterations=5000)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83999b1-ec48-47d0-bbf5-418821c25407",
   "metadata": {},
   "source": [
    "Evaluating the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa6dc2a-1770-4298-affe-2c58125a3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a9c1a3-73ee-476b-aa74-ab6954fe6a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on Test Set: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3824: RuntimeWarning: Mean of empty slice\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\rajpa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:142: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(predictions == y_test.flatten())\n",
    "print(f\"\\nAccuracy on Test Set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "966a80d4-27fb-4785-9dda-2e58ceec9ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by LogisticRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sk_model = LogisticRegression()\n\u001b[32m      3\u001b[39m sk_model.fit(X_train, y_train.flatten())\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sk_acc = \u001b[43msk_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSklearn Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msk_acc\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(accuracy - sk_acc) < \u001b[32m0.05\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:566\u001b[39m, in \u001b[36mClassifierMixin.score\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03mReturn :ref:`accuracy <accuracy_score>` on provided data and labels.\u001b[39;00m\n\u001b[32m    543\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m \u001b[33;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight=sample_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:387\u001b[39m, in \u001b[36mLinearClassifierMixin.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[32m    375\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    384\u001b[39m \u001b[33;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    386\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores.shape) == \u001b[32m1\u001b[39m:\n\u001b[32m    389\u001b[39m     indices = xp.astype(scores > \u001b[32m0\u001b[39m, indexing_dtype(xp))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:363\u001b[39m, in \u001b[36mLinearClassifierMixin.decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    360\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    361\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m coef_T = \u001b[38;5;28mself\u001b[39m.coef_.T \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coef_.ndim == \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coef_\n\u001b[32m    365\u001b[39m scores = safe_sparse_dot(X, coef_T, dense_output=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[38;5;28mself\u001b[39m.intercept_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2902\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2900\u001b[39m         out = X, y\n\u001b[32m   2901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1097\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1095\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1098\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1099\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1100\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1101\u001b[39m         )\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1104\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by LogisticRegression."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sk_model = LogisticRegression()\n",
    "sk_model.fit(X_train, y_train.flatten())\n",
    "sk_acc = sk_model.score(X_test, y_test.flatten())\n",
    "print(f\"Sklearn Accuracy: {sk_acc * 100:.2f}%\")\n",
    "\n",
    "if abs(accuracy - sk_acc) < 0.05:\n",
    "    print(\"SUCCESS: You matched the industry standard.\")\n",
    "else:\n",
    "    print(\"FAIL: Check your learning rate or normalization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367297f-d9fb-447d-9585-daa9dba432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a97cfe-fb56-48b4-9d19-cb7413377a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
